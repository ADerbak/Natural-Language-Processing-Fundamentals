{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the sentence, “I am reading Introduction to NLP”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = word_tokenize(\"The sky is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'sky', 'is', 'blue']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 : PoS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out parts of speech for each word of the sentence, “I am reading Introduction to NLP”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'Introduction', 'to', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"I am reading Introduction to NLP\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('reading', 'VBG'),\n",
       " ('Introduction', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('NLP', 'VB')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print list of stopwords provided by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 : Stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords from the given sentences:<br>\n",
    "“I am learning Python. It is one of the most popular programming  languages”<br>\n",
    "“Natural Language Processing deals with the art of extracting insights from Natural Languages”<br>\n",
    "“We are learning how to remove stop words automatically”<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I am learning Python. It is one of the most popular programming  languages\"\n",
    "sentence2 = \"Natural Language Processing deals with the art of extracting insights from Natural Languages\"\n",
    "sentence3 = \"We are learning how to remove stop words automatically\"\n",
    "sentence1_words = word_tokenize(sentence1)\n",
    "sentence2_words = word_tokenize(sentence2)\n",
    "sentence3_words = word_tokenize(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python . It one popular programming languages\n",
      "Natural Language Processing deals art extracting insights Natural Languages\n",
      "We learning remove stop words automatically\n"
     ]
    }
   ],
   "source": [
    "sentence1_no_stops = ' '.join([word for word in sentence1_words if word not in stop_words])\n",
    "print(sentence1_no_stops)\n",
    "sentence2_no_stops = ' '.join([word for word in sentence2_words if word not in stop_words])\n",
    "print(sentence2_no_stops)\n",
    "sentence3_no_stops = ' '.join([word for word in sentence3_words if word not in stop_words])\n",
    "print(sentence3_no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 : Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the text <br>\n",
    "**'I visited Allahabad from Calcutta on 22-10-18. Being born and brought up in a metro city like Kolkata, I always want to explore religious cities like Benares, Allahabad and Haridwar. Do you know the famous Kumbh Mela is being held at Prayagraj this year? I shall be attending Kumbh Mela on 21-01-19 and 19-02-19. Would you like to accompany me?'** <br>\n",
    "by replacing <br>\n",
    "'Benares' with 'Varanasi', <br>\n",
    "'Calcutta' with 'Kolkata', <br>\n",
    "'Allahabad' with 'Prayagraj' <br>\n",
    "and years in dates of format dd-mm-yy to yyyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited Prayagraj from Kolkata on 22-10-2018. Being born and brought up in a metro city like Kolkata, I always want to explore religious cities like Varanasi, Prayagraj and Haridwar. Do you know the famous Kumbh Mela is being held at Prayagraj this year? I shall be attending Kumbh Mela on 21-01-2019 and 19-02-2019. Would you like to accompany me?\n"
     ]
    }
   ],
   "source": [
    "sent = 'I visited Allahabad from Calcutta on 22-10-18. Being born and brought up in a metro city like Kolkata, \\\n",
    "I always want to explore religious cities like Benares, Allahabad and Haridwar. Do you know the famous Kumbh Mela \\\n",
    "is being held at Prayagraj this year? I shall be attending Kumbh Mela on 21-01-19 and 19-02-19. \\\n",
    "Would you like to accompany me?'\n",
    "normalized_sent = sent.replace('Benares','Varanasi').replace('Calcutta','Kolkata').replace('Allahabad','Prayagraj')\\\n",
    ".replace('-18','-2018').replace('-19','-2019')\n",
    "\n",
    "print(normalized_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 : Spelling correction of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct the spelling of the word ‘Natureal’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell('Natureal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 : Spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You have the following sentences which contain spelling mistakes. You need to correct them using Python. <br>\n",
    "“I am laerning Python which is one of the mostt populer programmming  langages” <br>\n",
    "“Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes” <br>\n",
    "“We are learning how to crroect spellinges autametically”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I am laerning Python which is one of the mostt populer programmming  langages\"\n",
    "sentence2 = \"Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes\"\n",
    "sentence3 = \"We are learning how to crroect spellinges autametically\"\n",
    "sentence1_words = word_tokenize(sentence1)\n",
    "sentence2_words = word_tokenize(sentence2)\n",
    "sentence3_words = word_tokenize(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning Python which is one of the most popular programming languages\n",
      "Natural Language Procession deals with the art of extracting insights from Natural Languages\n",
      "We are learning how to correct spellings automatically\n"
     ]
    }
   ],
   "source": [
    "sentence1_corrected = ' '.join([spell(word) for word in sentence1_words])\n",
    "print(sentence1_corrected)\n",
    "sentence2_corrected = ' '.join([spell(word) for word in sentence2_words])\n",
    "print(sentence2_corrected)\n",
    "sentence3_corrected = ' '.join([spell(word) for word in sentence3_words])\n",
    "print(sentence3_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply stemming on the following words: <br>\n",
    "production<br>\n",
    "coming<br>\n",
    "firing<br>\n",
    "battling<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmer.stem('production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('coming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fire'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('firing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('battling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise  9 : Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize the following words: <br>\n",
    "products <br>\n",
    "production <br>\n",
    "coming <br>\n",
    "firing <br>\n",
    "battle <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'production'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coming'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('coming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'firing'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('firing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battle'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('battle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 : Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out named entities from the sentence \"We are reading a book published by Packt which is based out of Birmingham.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "sentence = \"We are reading a book published by Packt which is based out of Birmingham.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NE', [('Packt', 'NNP')]), Tree('NE', [('Birmingham', 'NNP')])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)), binary=True)\n",
    "[a for a in i if len(a)==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 :  Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the sense of the word ‘bank’ in the following two sentences:<br>\n",
    "Keep your savings in the bank<br>\n",
    "It's so risky to drive over the banks of the road<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n",
      "Synset('bank.v.07')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "sentence1 = \"Keep your savings in the bank\"\n",
    "print(lesk(word_tokenize(sentence1), 'bank'))\n",
    "sentence = \"It's so risky to drive over the banks of the road\"\n",
    "print(lesk(word_tokenize(sentence), 'bank'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synset('savings_bank.n.02') refers to a container for keeping money safely at home <br>\n",
    "Synset('bank.v.07') refers to a slope in the turn of a road"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12 : Sentence Boundary Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract sentences from the text <br>\n",
    "“We are reading a book. Do you know who is the publisher? It is Packt. Packt is based out of Birmingham.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are reading a book.',\n",
       " 'Do you know who is the publisher?',\n",
       " 'It is Packt.',\n",
       " 'Packt is based out of Birmingham.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(\"We are reading a book. Do you know who is the publisher? It is Packt. Packt is based out of Birmingham.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1 : Preprocessing of raw texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following tasks:\n",
    "\n",
    "Tokenization,\n",
    "Spelling correction,\n",
    "Pos tagging,\n",
    "Stop words removal,\n",
    "Stemming,\n",
    "Lemmatization,\n",
    "Named Entity Recognition,\n",
    "Text normalisation,\n",
    "Word Sense Disambiguation,\n",
    "Sentence Boundary Detection,\n",
    "\n",
    "On the following text corpus:\n",
    "\n",
    "In this book, we shall lerning how to pracess Natueral Language and extract insights from it. The first four chapter will introduce you to the basics of NLP. Later chapters will describe how to deal with complex NLP prajects. If you want to get early access of it, you should book your order now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import spell\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"In this book authored by Sohom Ghosh and Dwight Gunning, we shall learnning how to pracess \\\n",
    "Natueral Language and extract insights from it. The first four chapter will introduce you to the basics of NLP. \\\n",
    "Later chapters will describe how to deal with complex NLP prajects. If you want to get early access of it, you \\\n",
    "should book your order now.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'by',\n",
       " 'Sohom',\n",
       " 'Ghosh',\n",
       " 'and',\n",
       " 'Dwight',\n",
       " 'Gunning',\n",
       " ',',\n",
       " 'we',\n",
       " 'shall',\n",
       " 'learnning',\n",
       " 'how',\n",
       " 'to',\n",
       " 'pracess',\n",
       " 'Natueral',\n",
       " 'Language',\n",
       " 'and']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "words[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sohom has been corrected to: Soho\n",
      "Ghosh has been corrected to: Ghost\n",
      "learnning has been corrected to: learning\n",
      "pracess has been corrected to: process\n",
      "Natueral has been corrected to: Natural\n",
      "prajects has been corrected to: projects\n"
     ]
    }
   ],
   "source": [
    "corrected_sentence = \"\"\n",
    "corrected_word_list = []\n",
    "for wd in words:\n",
    "    if wd not in string.punctuation:\n",
    "        wd_c = spell(wd)\n",
    "        if wd_c != wd:\n",
    "            print(wd + \" has been corrected to: \" + wd_c)\n",
    "            corrected_sentence = corrected_sentence + ' ' +wd_c\n",
    "            corrected_word_list.append(wd_c)\n",
    "        else:\n",
    "            corrected_sentence = corrected_sentence + ' ' + wd\n",
    "            corrected_word_list.append(wd)\n",
    "    else:\n",
    "        corrected_sentence = corrected_sentence + wd\n",
    "        corrected_word_list.append(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In this book authored by Soho Ghost and Dwight Gunning, we shall learning how to process Natural Language and extract insights from it. The first four chapter will introduce you to the basics of NLP. Later chapters will describe how to deal with complex NLP projects. If you want to get early access of it, you should book your order now.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'by',\n",
       " 'Soho',\n",
       " 'Ghost',\n",
       " 'and',\n",
       " 'Dwight',\n",
       " 'Gunning',\n",
       " ',',\n",
       " 'we',\n",
       " 'shall',\n",
       " 'learning',\n",
       " 'how',\n",
       " 'to',\n",
       " 'process',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'and']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_word_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('book', 'NN'),\n",
       " ('authored', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Soho', 'NNP'),\n",
       " ('Ghost', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Dwight', 'NNP'),\n",
       " ('Gunning', 'NNP'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('shall', 'MD'),\n",
       " ('learning', 'VB'),\n",
       " ('how', 'WRB'),\n",
       " ('to', 'TO'),\n",
       " ('process', 'VB'),\n",
       " ('Natural', 'NNP'),\n",
       " ('Language', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('extract', 'JJ'),\n",
       " ('insights', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('first', 'JJ'),\n",
       " ('four', 'CD'),\n",
       " ('chapter', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('introduce', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('basics', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Later', 'NNP'),\n",
       " ('chapters', 'NNS'),\n",
       " ('will', 'MD'),\n",
       " ('describe', 'VB'),\n",
       " ('how', 'WRB'),\n",
       " ('to', 'TO'),\n",
       " ('deal', 'VB'),\n",
       " ('with', 'IN'),\n",
       " ('complex', 'JJ'),\n",
       " ('NLP', 'NNP'),\n",
       " ('projects', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('If', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('get', 'VB'),\n",
       " ('early', 'JJ'),\n",
       " ('access', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " (',', ','),\n",
       " ('you', 'PRP'),\n",
       " ('should', 'MD'),\n",
       " ('book', 'NN'),\n",
       " ('your', 'PRP$'),\n",
       " ('order', 'NN'),\n",
       " ('now', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(corrected_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'Soho',\n",
       " 'Ghost',\n",
       " 'Dwight',\n",
       " 'Gunning',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learning',\n",
       " 'process',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'extract',\n",
       " 'insights',\n",
       " '.',\n",
       " 'The',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "corrected_word_list_without_stopwords = []\n",
    "for wd in corrected_word_list:\n",
    "    if wd not in stop_words:\n",
    "        corrected_word_list_without_stopwords.append(wd)\n",
    "corrected_word_list_without_stopwords[:20]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'author',\n",
       " 'soho',\n",
       " 'ghost',\n",
       " 'dwight',\n",
       " 'gun',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learn',\n",
       " 'process',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'extract',\n",
       " 'insight',\n",
       " '.',\n",
       " 'the',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "corrected_word_list_without_stopwords_stemmed = []\n",
    "for wd in corrected_word_list_without_stopwords:\n",
    "    corrected_word_list_without_stopwords_stemmed.append(stemmer.stem(wd))\n",
    "corrected_word_list_without_stopwords_stemmed[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'Soho',\n",
       " 'Ghost',\n",
       " 'Dwight',\n",
       " 'Gunning',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learning',\n",
       " 'process',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'extract',\n",
       " 'insight',\n",
       " '.',\n",
       " 'The',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "corrected_word_list_without_stopwords_lemmatized = []\n",
    "for wd in corrected_word_list_without_stopwords:\n",
    "    corrected_word_list_without_stopwords_lemmatized.append(lemmatizer.lemmatize(wd))\n",
    "corrected_word_list_without_stopwords_lemmatized[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' In this book authored by Soho Ghost and Dwight Gunning, we shall learning how to process Natural Language and extract insights from it.',\n",
       " 'The first four chapter will introduce you to the basics of NLP.',\n",
       " 'Later chapters will describe how to deal with complex NLP projects.',\n",
       " 'If you want to get early access of it, you should book your order now.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corrected_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2 : Basic Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following task on the sentence, 'The quick brown fox jumps over the lazy dog' <br>\n",
    "1) Verify if the word 'quick' belongs to it <br>\n",
    "2) Find the starting position of the word 'fox' in the sentence. (Assume 1st character to be at position 0, 2nd character to be at position 1 and so on) <br>\n",
    "3) Find the rank of word 'lazy' in the sentence (Consider the first word to have rank 0, 2nd word to have rank 1 and so on) <br>\n",
    "4) Print the 3rd word of the sentence <br>\n",
    "5) Print the 3rd word of the sentence in reverse order <br>\n",
    "6) Concatenate the first and last words of the sentence <br>\n",
    "7) Print words at even positions of the sentence (Assume first word is at position 0, 2nd word is at position 1 and so on) <br>\n",
    "8) Print 3 letters from the end of the sentence <br>\n",
    "9) Print the sentence in reverse order <br>\n",
    "10) Print the first 4 letters of the sentence <br>\n",
    "11) Print each words of the sentence in proper sequence but in reverse order <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The quick brown fox jumps over the lazy dog' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1 solution\n",
    "'quick' in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2 solution\n",
    "sentence.index('fox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3 solution\n",
    "sentence.split().index('lazy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brown'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4 solution\n",
    "sentence.split()[2]\n",
    "#3rd word is the word at 2nd position as counting in Python beings from 0 (zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nworb'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5 solution\n",
    "sentence.split()[2][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thedog\n"
     ]
    }
   ],
   "source": [
    "#Q6 solution\n",
    "words = sentence.split()\n",
    "first_word = words[0]\n",
    "last_word = words[len(words)-1]\n",
    "concat_word = first_word + last_word\n",
    "print(concat_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'brown', 'jumps', 'the', 'dog']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7 solution\n",
    "[words[i] for i in range(len(words)) if i%2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8 solution\n",
    "sentence[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'god yzal eht revo spmuj xof nworb kciuq ehT'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q9 solution\n",
    "sentence[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q10 solution\n",
    "sentence[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ehT kciuq nworb xof spmuj revo eht yzal god\n"
     ]
    }
   ],
   "source": [
    "#Q11 solution\n",
    "print(' '.join([word[::-1] for word in words]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
