{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = word_tokenize(\"The sky is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'sky', 'is', 'blue']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'Introduction', 'to', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"I am reading Introduction to NLP\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('sky', 'NN'), ('is', 'VBZ'), ('blue', 'JJ')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('reading', 'VBG'),\n",
       " ('Introduction', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('NLP', 'VB')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'production'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('production')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmer.stem('production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fire'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('firing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'firing'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('firing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battl'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('battling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battle'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('battle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in /Users/sohom.ghosh/anaconda3/lib/python3.7/site-packages (0.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell('Natureal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I am laerning Python which is one of the mostt populer programmming  langages\"\n",
    "sentence2 = \"Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes\"\n",
    "sentence3 = \"We are learning how to crroect spellinges autametically\"\n",
    "sentence1_words = word_tokenize(sentence1)\n",
    "sentence2_words = word_tokenize(sentence2)\n",
    "sentence3_words = word_tokenize(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning Python which is one of the most popular programming languages\n",
      "Natural Language Procession deals with the art of extracting insights from Natural Languages\n",
      "We are learning how to correct spellings automatically\n"
     ]
    }
   ],
   "source": [
    "sentence1_corrected = ' '.join([spell(word) for word in sentence1_words])\n",
    "print(sentence1_corrected)\n",
    "sentence2_corrected = ' '.join([spell(word) for word in sentence2_words])\n",
    "print(sentence2_corrected)\n",
    "sentence3_corrected = ' '.join([spell(word) for word in sentence3_words])\n",
    "print(sentence3_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity : Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I am learning Python. It is one of the most popular programming  languages\"\n",
    "sentence2 = \"Natural Language Processing deals with the art of extracting insights from Natural Languages\"\n",
    "sentence3 = \"We are learning how to remove stop words automatically\"\n",
    "sentence1_words = word_tokenize(sentence1)\n",
    "sentence2_words = word_tokenize(sentence2)\n",
    "sentence3_words = word_tokenize(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python . It one popular programming languages\n",
      "Natural Language Processing deals art extracting insights Natural Languages\n",
      "We learning remove stop words automatically\n"
     ]
    }
   ],
   "source": [
    "sentence1_no_stops = ' '.join([word for word in sentence1_words if word not in stop_words])\n",
    "print(sentence1_no_stops)\n",
    "sentence2_no_stops = ' '.join([word for word in sentence2_words if word not in stop_words])\n",
    "print(sentence2_no_stops)\n",
    "sentence3_no_stops = ' '.join([word for word in sentence3_words if word not in stop_words])\n",
    "print(sentence3_no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/sohom.ghosh/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "sentence = \"We are reading a book published by Packt which is based out of Birmingham.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NE', [('Packt', 'NNP')]), Tree('NE', [('Birmingham', 'NNP')])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)), binary=True)\n",
    "[a for a in i if len(a)==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n",
      "Synset('bank.v.07')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "sentence1 = \"Keep your savings in the bank\"\n",
    "print(lesk(word_tokenize(sentence1), 'bank'))\n",
    "sentence = \"It's so risky to drive over the banks of the road\"\n",
    "print(lesk(word_tokenize(sentence), 'bank'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synset('savings_bank.n.02') refers to a container for keeping money safely at home <br>\n",
    "Synset('bank.v.07') refers to a slope in the turn of a road"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Boundary Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are reading a book.',\n",
       " 'Do you know who is the publisher?',\n",
       " 'It is Packt.',\n",
       " 'Packt is based out of Birmingham.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(\"We are reading a book. Do you know who is the publisher? It is Packt. Packt is based out of Birmingham.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the following tasks:\n",
    "Tokenization,\n",
    "Spelling correction,\n",
    "Pos tagging,\n",
    "Stop words removal,\n",
    "Stemming,\n",
    "Lemmatization,\n",
    "Named Entity Recognition,\n",
    "Text normalisation,\n",
    "Word Sense Disambiguation,\n",
    "Sentence Boundary Detection,\n",
    "\n",
    "On the following text corpus:\n",
    "\n",
    "In this book, we shall lerning how to pracess Natueral Language and extract insights from it. The first four chapter will introduce you to the basics of NLP. Later chapters will describe how to deal with complex NLP prajects. If you want to get early access of it, you should book your order now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import spell\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"In this book authored by Sohom Ghosh and Dwight Gunning, we shall learnning how to pracess \\\n",
    "Natueral Language and extract insights from it. The first four chapter will introduce you to the basics of NLP. \\\n",
    "Later chapters will describe how to deal with complex NLP prajects. If you want to get early access of it, you \\\n",
    "should book your order now.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'by',\n",
       " 'Sohom',\n",
       " 'Ghosh',\n",
       " 'and',\n",
       " 'Dwight',\n",
       " 'Gunning',\n",
       " ',',\n",
       " 'we',\n",
       " 'shall',\n",
       " 'learnning',\n",
       " 'how',\n",
       " 'to',\n",
       " 'pracess',\n",
       " 'Natueral',\n",
       " 'Language',\n",
       " 'and']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "words[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sohom has been corrected to: Soho\n",
      "Ghosh has been corrected to: Ghost\n",
      "learnning has been corrected to: learning\n",
      "pracess has been corrected to: process\n",
      "Natueral has been corrected to: Natural\n",
      "prajects has been corrected to: projects\n"
     ]
    }
   ],
   "source": [
    "corrected_sentence = \"\"\n",
    "corrected_word_list = []\n",
    "for wd in words:\n",
    "    if wd not in string.punctuation:\n",
    "        wd_c = spell(wd)\n",
    "        if wd_c != wd:\n",
    "            print(wd + \" has been corrected to: \" + wd_c)\n",
    "            corrected_sentence = corrected_sentence + ' ' +wd_c\n",
    "            corrected_word_list.append(wd_c)\n",
    "        else:\n",
    "            corrected_sentence = corrected_sentence + ' ' + wd\n",
    "            corrected_word_list.append(wd)\n",
    "    else:\n",
    "        corrected_sentence = corrected_sentence + wd\n",
    "        corrected_word_list.append(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In this book authored by Soho Ghost and Dwight Gunning, we shall learning how to process Natural Language and extract insights from it. The first four chapter will introduce you to the basics of NLP. Later chapters will describe how to deal with complex NLP projects. If you want to get early access of it, you should book your order now.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'by',\n",
       " 'Soho',\n",
       " 'Ghost',\n",
       " 'and',\n",
       " 'Dwight',\n",
       " 'Gunning',\n",
       " ',',\n",
       " 'we',\n",
       " 'shall',\n",
       " 'learning',\n",
       " 'how',\n",
       " 'to',\n",
       " 'process',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'and']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_word_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('book', 'NN'),\n",
       " ('authored', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Soho', 'NNP'),\n",
       " ('Ghost', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Dwight', 'NNP'),\n",
       " ('Gunning', 'NNP'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('shall', 'MD'),\n",
       " ('learning', 'VB'),\n",
       " ('how', 'WRB'),\n",
       " ('to', 'TO'),\n",
       " ('process', 'VB'),\n",
       " ('Natural', 'NNP'),\n",
       " ('Language', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('extract', 'JJ'),\n",
       " ('insights', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('first', 'JJ'),\n",
       " ('four', 'CD'),\n",
       " ('chapter', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('introduce', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('basics', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Later', 'NNP'),\n",
       " ('chapters', 'NNS'),\n",
       " ('will', 'MD'),\n",
       " ('describe', 'VB'),\n",
       " ('how', 'WRB'),\n",
       " ('to', 'TO'),\n",
       " ('deal', 'VB'),\n",
       " ('with', 'IN'),\n",
       " ('complex', 'JJ'),\n",
       " ('NLP', 'NNP'),\n",
       " ('projects', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('If', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('get', 'VB'),\n",
       " ('early', 'JJ'),\n",
       " ('access', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " (',', ','),\n",
       " ('you', 'PRP'),\n",
       " ('should', 'MD'),\n",
       " ('book', 'NN'),\n",
       " ('your', 'PRP$'),\n",
       " ('order', 'NN'),\n",
       " ('now', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(corrected_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'Soho',\n",
       " 'Ghost',\n",
       " 'Dwight',\n",
       " 'Gunning',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learning',\n",
       " 'process',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'extract',\n",
       " 'insights',\n",
       " '.',\n",
       " 'The',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "corrected_word_list_without_stopwords = []\n",
    "for wd in corrected_word_list:\n",
    "    if wd not in stop_words:\n",
    "        corrected_word_list_without_stopwords.append(wd)\n",
    "corrected_word_list_without_stopwords[:20]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'author',\n",
       " 'soho',\n",
       " 'ghost',\n",
       " 'dwight',\n",
       " 'gun',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learn',\n",
       " 'process',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'extract',\n",
       " 'insight',\n",
       " '.',\n",
       " 'the',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "corrected_word_list_without_stopwords_stemmed = []\n",
    "for wd in corrected_word_list_without_stopwords:\n",
    "    corrected_word_list_without_stopwords_stemmed.append(stemmer.stem(wd))\n",
    "corrected_word_list_without_stopwords_stemmed[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'Soho',\n",
       " 'Ghost',\n",
       " 'Dwight',\n",
       " 'Gunning',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learning',\n",
       " 'process',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'extract',\n",
       " 'insight',\n",
       " '.',\n",
       " 'The',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "corrected_word_list_without_stopwords_lemmatized = []\n",
    "for wd in corrected_word_list_without_stopwords:\n",
    "    corrected_word_list_without_stopwords_lemmatized.append(lemmatizer.lemmatize(wd))\n",
    "corrected_word_list_without_stopwords_lemmatized[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' In this book authored by Soho Ghost and Dwight Gunning, we shall learning how to process Natural Language and extract insights from it.',\n",
       " 'The first four chapter will introduce you to the basics of NLP.',\n",
       " 'Later chapters will describe how to deal with complex NLP projects.',\n",
       " 'If you want to get early access of it, you should book your order now.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corrected_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
