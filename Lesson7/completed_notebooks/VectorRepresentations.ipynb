{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Machine learning algorithms work by detecting patterns in your data and so prefer the data to be structured, as opposed to say text - which is relatively unstructured. So - a necessary processing step in Natural Language processing is the representation of text in a structured way. Representing text as mathematical structures allow algorithms to perform mathematical functions on text and in so doing \"learn\" the meaning of text. \n",
    "\n",
    "The types and forms of the structures depend on the type and scope of the NLP project you are doing. If you were to undertake a project to understand reddit posts then you would need to be able to represent all the possible words that can be contained in all the posts in your project. You may also want to decide if your are more concerned about representing each post separately, especially if you are attempting to classify posts or if your project is more concerned with how words relate to each other as in a project to detect named entities.\n",
    "\n",
    "This chapter looks at how to represent text so as to make it easier to machine learning algorithims to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levels of Representation\n",
    "Text can be represented at different levels of granularity - (1) character (2) word (3) sentences (4) paragraphs (5) documents. Let's start at each level to see how each can be represented as vectors, bearing in mind that each level is a superset of the level that precedes it i.e. words contain characters, sentences contain words and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Load A Text File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Representation\n",
    "The goal of character representation is to find a way to represent characters numerically before performing machine learning. The simplest way is to take use an existing numeric encoding of characters - such as the Ascii charset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 66)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('A'), ord('B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However because the encoded numeric values are increasing - it contains information about relative scale of the values - giving the impression that A - coded as 65 is less than B - coded as 66. On-hot encoding gets aroud these problems by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "chars = string.ascii_letters + '.,- '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "char_value = {c:i for i,c in enumerate(chars)}\n",
    "\n",
    "def encode_char(c):\n",
    "    encoded_char = [0] * len(chars)\n",
    "    encoded_char[char_value[c]] =1\n",
    "    return encoded_char\n",
    "\n",
    "def encode_word(word):\n",
    "    return[encode_char(c) for c in word]\n",
    "\n",
    "#encode_word('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.ascii_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: One Hot Encoding using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['d' 'a' 't' 'a']]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only 2 non-keyword arguments accepted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f0ac5df15354>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_chars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: only 2 non-keyword arguments accepted"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "word = 'data'\n",
    "word_chars = np.array(list(word)).reshape(1, -1)\n",
    "print(word_chars)\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', n_values=10)\n",
    "encoder.fit_transform(np.array(1,3,5).reshape(1,-1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: One Hot Encoding Text using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how characters can be represented let's now look at how to represent words as vectors. You can use one-hot encoding for words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have represented words as one-hot vectors One problem with one-hot vectors is that with only 1 position filled with a 1 and the rest 0's the vector is incedibly sparse. This gives us a representation that takes up a lot of memory and computation time to represent one word and is exspecially an issue with large vocabularies. For example - with a vocabulary size of 10000 each one-hot vector takes 10000 * bytes of memory - although you can reduce this by using sparse vectors.\n",
    "\n",
    "The other problem with one-hot encoding is that it doesn't make efficient use of memory to carry information - since every other byte but one is empty.\n",
    "\n",
    "It turns out that there is another transformation that we can do to those vectors that can give use an even better representation of words -  a representation that both reduces meory usage compares and carries more information compared to one-hot encoding.\n",
    "\n",
    "An embedding vector is a vector that is transformed from a high dimension to a lower dimension. So if you had one-hot vectors of size 10000 you can compress that vector into a smaller dimension - for example 100.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80064, 864)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "getsizeof([1]*10000), getsizeof([1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Word2Vec is an approach to training shallow neural networks to produce word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dwight\\Anaconda3\\envs\\packt\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "corpus = [\n",
    "          'Text of the first document.',\n",
    "          'Text of the second document made longer.',\n",
    "          'Number three.',\n",
    "          'This is number four.',\n",
    "]\n",
    "# we need to pass splitted sentences to the model\n",
    "tokenized_sentences = [sentence.split() for sentence in corpus]\n",
    "model = word2vec.Word2Vec(tokenized_sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main methods for creeating embeddings using the Word2Vec approach - Continuous Bag of Words and Skip Grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "Word2Vec works nicely as embeddings for individual words but what if we need to create embeddings for paragraphs or documents. This is where Doc2Vec comes in - it is a way of creating embeddings for groups of words.\n",
    "\n",
    "Doc2Vec is based on the concept of a paragraph vector - which is a vector that gets trained to capture the meaning of a paragraph or document. Like Word2Vec, it is also trained using a shallow neural network and it enjoys much the same properties as word vectors. If trained properly paragraphs or documents that have similar meaining will have similar document vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Training Document Vectors\n",
    "\n",
    "In this exercise we will learn how to train document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def load_corpus(fn):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:packt]",
   "language": "python",
   "name": "conda-env-packt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
